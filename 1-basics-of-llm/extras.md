# SUGGESTED READINGS AND RESOURCES

## Blog Posts (Less Technical)

### [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- Blog post explaining the transformer architecture.

### [Illustrated GPT](http://jalammar.github.io/illustrated-gpt2/)
- Blog post explaining GPT models.

## Academic Papers

For those who want to know more about how the architecture and models evolved:

### [Attention is All You Need (2017)](https://arxiv.org/abs/1706.03762)
- Paper that introduced the transformer architecture; almost all state-of-the-art models are based on this architecture.

### [BERT (2018)](https://arxiv.org/abs/1810.04805)
- Showed that self-supervised pre-training significantly helps in downstream NLP tasks.

## Videos

Andrej Karpathy's videos are great:

### [State of GPT](https://www.youtube.com/watch?v=bZQun8Y4L2A)
- Video explaining the state and advancements of GPT models.

### [The Backpropagation Algorithm](https://www.youtube.com/watch?v=i94OvYb6noo)
- Video detailing the backpropagation algorithm.

### [Let's Build GPT: From Scratch, In Code, Spelled Out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- Video tutorial on building GPT from scratch in code.
